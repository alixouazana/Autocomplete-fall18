Alix Ouazana (aro16)
*** Note to Grader: My computer is in French so all periods are replaced by commas ***

(1) Run the program BenchmarkForAutocomplete and copy/paste the 
results here this for #matches = 20

search	size	#match	binary	brute
	    456976	20	    0,3075	0,0089
a	    17576	20	    0,0047	0,0147
b	    17576	20	    0,0039	0,0066
c	    17576	20	    0,0032	0,0071
x	    17576	20	    0,0034	0,0034
y	    17576	20	    0,0031	0,0034
z	    17576	20	    0,0030	0,0036
aa	    676	    20	    0,0001	0,0051
az	    676	    20	    0,0001	0,0038
za	    676	    20	    0,0001	0,0062
zz	    676	    20	    0,0002	0,0052



(2) Run the program again for #matches = 10000, paste the results, 
and then make any conclusions about how the # matches 
effects the runtime. 

search	size	#match	binary	brute
	    456976	10000	0,3557	0,0278
a	    17576	10000	0,0031	0,0266
b	    17576	10000	0,0032	0,0094
c	    17576	10000	0,0031	0,0085
x	    17576	10000	0,0034	0,0100
y	    17576	10000	0,0036	0,0107
z	    17576	10000	0,0041	0,0122
aa	    676	    10000	0,0002	0,0082
az	    676	    10000	0,0002	0,0065
za	    676	    10000	0,0001	0,0054
zz	    676	    10000	0,0001	0,0050

The number of matches doesn't seem to effect runtime, as the same searches and size seem to generate the same runtime for both 
the binary and brute timings. However, the size will affect runtime.

(3) Copy/paste the code from BruteAutocomplete.topMatches below. 
Explain what the Big-Oh complexity of the entire loop: 
for(Term t : myTerms) {...} 
is in terms of N, the number of elements in myTerms and 
M, the number of terms that match the prefix. 
Assume that every priority-queue operation runs in O(log k) time. 
Explain your answer which should be in terms of N, M, and k.

public List<Term> topMatches(String prefix, int k) {
		if (k < 0) {
			throw new IllegalArgumentException("Illegal value of k:"+k);
		}
		
		// maintain pq of size k
		PriorityQueue<Term> pq = new PriorityQueue<Term>(10, new Term.WeightOrder());
		for (Term t : myTerms) {
			if (!t.getWord().startsWith(prefix))
				continue;
			if (pq.size() < k) {
				pq.add(t);
			} else if (pq.peek().getWeight() < t.getWeight()) {
				pq.remove();
				pq.add(t);
			}
		}
		int numResults = Math.min(k, pq.size());
		LinkedList<Term> ret = new LinkedList<>();
		for (int i = 0; i < numResults; i++) {
			ret.addFirst(pq.remove());
		}
		return ret;
	}


The for loop "for(Term t : myTerms)" will run N times, as it will go through myTerms term by term. Every operation in the loop is O(log k). 
This means the loop will run O(N log k) because every log k operation will be run N times.  


(4) Explain why the last for loop in BruteAutocomplete.topMatches 
uses a LinkedList (and not an ArrayList) 
AND why the PriorityQueue uses Term.WeightOrder to get 
the top k heaviest matches -- rather than 
using Term.ReverseWeightOrder.

The last for loop in BruteAutocomplete.topMatches uses a LinkedList because it adds to the front therefore adding
to the front of a LinkedList is O(1) because you just need to point the value to the previous first one whereas
adding to the front of an ArrayList is O(N) (assuming the length of the ArrayList is N) because you need to shift the rest of the list 
by one index. 
The PriorityQueue uses Term.WeightOrder to get the top k heaviest matches because it is FIFO (first in first out) therefore removing 
with pq.remove() removes the first value and not the last one. With Term.Weightorder you add the next, heavier weight at the beginning of 
the queue each time therefore the first value is already the heaviest. Adding all values from .remove adds them to the list in descending order. 
Therefore Term.ReverseWeightOrder would give a list with the weights in ascending order which is not what we want. 


(5) Explain what the runtime of the 
BinarySearchAutocomplete.topMatches code that you 
implemented is by copy/pasting the code below 
and explaining your answer in terms of N, M, and k.

*** All the runtimes are shown next to the line of code for simplicity***

public List<Term> topMatches(String prefix, int k) {
		ArrayList<Term> list = new ArrayList<>();
		if (prefix == null) {
			throw new NullPointerException("null prefix "+ prefix);
		}
		
		Term targetkey = new Term(prefix, 0);
		Comparator<Term> comp = new Term.PrefixOrder(prefix.length());
		int firstindex = firstIndexOf(myTerms, targetkey, comp); ///// Runtime; 1 + O(log N)
		int lastindex = lastIndexOf(myTerms, targetkey, comp); ///// Runtime; 1 + O(log N)
		if(firstindex == -1 || lastindex == -1) return list;
		for(int k1 = firstindex; k1<= lastindex; k1++) { //// O(M)
			list.add(myTerms[k1]);
		}
		
		Collections.sort(list, new Term.ReverseWeightOrder()); /// O(M log M)
		if(list.size()> k) return list.subList(0, k); //// O(k)
		
		return list;
	}


Runtime for first and last index is both 1 + O(log N) so summing and reducing both gives O(log N). 
This is because the length of the array is N so finding the first and last occurences of an object with efficient Binary search with
an interval (which is what we did) is O(log N).
M is the number of matches, which is the number of occurrences of the target, which is the size of the interval from firstindex to lastindex. 
Therefore looping through all the matches is O(M).
Sorting worst case with collections.sort for a size M (number of matches, from first to last index) is O(M log M). 
Adding up O(M) and O(M log M) gives O(M log M). 
Finally getting a list of size k which is a sublist of an already made list, is O(k). However, k is usually 10 or a small number, 
much smaller than M, so can be neglected (even when relatively big which is why the priority queue program uses 10). 
Therefore the total runtime is O(log N) + O(M log M) = O(log N + M log M). 
Therefore the total runtime of BinarySearchAutocomplete.topMatches is O(log N + M log M).





 
